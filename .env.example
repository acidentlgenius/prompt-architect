# ==============================================================================
# Prompt Architect Configuration
# ==============================================================================
# INSTRUCTIONS:
# 1. Copy this file to .env: `cp .env.example .env`
# 2. Choose your backend: 'local' (Ollama) or 'cloud' (Groq/Gemini/OpenAI)
# 3. Fill in the required values for your chosen backend.
#
# IMPORTANT: DO NOT COMMIT YOUR ACTUAL .env FILE TO VERSION CONTROL.
# ==============================================================================

# Common Configuration
PROMPT_ARCHITECT_BACKEND=local

# ------------------------------------------------------------------------------
# LOCAL BACKEND (Ollama)
# ------------------------------------------------------------------------------
# URL for Ollama's OpenAI-compatible endpoint
PROMPT_ARCHITECT_LOCAL_URL=http://localhost:11434/v1
# The model you have pulled in Ollama (e.g., llama3, mistral, codellama)
PROMPT_ARCHITECT_MODEL=llama3

# ------------------------------------------------------------------------------
# CLOUD BACKEND (Groq / Gemini / OpenAI / etc.)
# ------------------------------------------------------------------------------
# Set PROMPT_ARCHITECT_BACKEND=cloud to use these settings.
# PROMPT_ARCHITECT_API_KEY=YOUR_API_KEY_HERE
# PROMPT_ARCHITECT_API_URL=https://api.groq.com/openai/v1
# PROMPT_ARCHITECT_MODEL=llama3-70b-8192

# Gemini Example:
# PROMPT_ARCHITECT_API_URL=https://generativelanguage.googleapis.com/v1beta/openai/
# PROMPT_ARCHITECT_MODEL=gemini-2.0-flash-exp
